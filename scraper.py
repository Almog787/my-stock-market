import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
import pytz
import pandas as pd
import re

# ×”×’×“×¨×•×ª ×§×‘×¦×™×
URLS_FILE = "urls.txt"
DATA_FILE = "data.json"
README_FILE = "README.md"
TZ_ISRAEL = pytz.timezone('Asia/Jerusalem')

def get_product_data(product_url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept-Language': 'he-IL,he;q=0.9,en-US;q=0.8,en;q=0.7'
    }
    
    try:
        response = requests.get(product_url, headers=headers, timeout=20)
        response.raise_for_status()
        soup = BeautifulSoup(response.content, 'html.parser')
        
        price = None
        title = None

        # --- ×©×œ×‘ 1: ×©×œ×™×¤×ª ×›×•×ª×¨×ª ---
        # × × ×¡×” ×§×•×“× ××˜×-×“××˜×” (×”×›×™ ×××™×Ÿ)
        title_meta = soup.find("meta", property="og:title") or soup.find("meta", dict(name="title"))
        if title_meta:
            title = title_meta["content"]
        else:
            title_tag = soup.find('h1')
            title = title_tag.get_text(strip=True) if title_tag else "××•×¦×¨ ×œ×œ× ×©×"

        # --- ×©×œ×‘ 2: ×©×œ×™×¤×ª ××—×™×¨ (×©×™×˜×” ×’× ×¨×™×ª ×—×›××”) ---
        
        # ×. ×—×™×¤×•×© ×‘××˜×-×“××˜×” ×©×œ ××—×™×¨ (× ×¤×•×¥ ×××•×“ ×‘××ª×¨×™× ××§×¦×•×¢×™×™×)
        price_meta = (
            soup.find("meta", property="product:price:amount") or 
            soup.find("meta", property="og:price:amount") or
            soup.find("meta", dict(name="twitter:data1")) # ×œ×¢×™×ª×™× ×”××—×™×¨ ×›××Ÿ
        )
        if price_meta:
            price = price_meta["content"]

        # ×‘. ×× ×œ× × ××¦×, ×—×™×¤×•×© ×‘-JSON-LD (×¤×•×¨××˜ × ×ª×•× ×™× ×©×œ ×’×•×’×œ ×©× ××¦× ×‘×¨×•×‘ ×”××ª×¨×™×)
        if not price:
            scripts = soup.find_all('script', type='application/ld+json')
            for script in scripts:
                try:
                    json_data = json.loads(script.string)
                    # ××—×¤×© ××ª ×©×“×” ×”××—×™×¨ ×‘×ª×•×š ××‘× ×” ×’××™×©
                    if isinstance(json_data, dict):
                        offers = json_data.get('offers')
                        if isinstance(offers, dict):
                            price = offers.get('price')
                        elif isinstance(offers, list):
                            price = offers[0].get('price')
                    if price: break
                except:
                    continue

        # ×’. ×’×™×‘×•×™ ××—×¨×•×Ÿ: ×—×™×¤×•×© ×ª×’×™×•×ª HTML × ×¤×•×¦×•×ª ×œ××—×™×¨
        if not price:
            # ××—×¤×© ××œ×× ×˜×™× ×©××›×™×œ×™× class ×¢× ×”××™×œ×” price
            price_elements = soup.find_all(class_=re.compile(r'price|final-price|current-price', re.I))
            for elem in price_elements:
                text = elem.get_text(strip=True)
                # ××—×œ×¥ ×¨×§ ××¡×¤×¨×™× ×•× ×§×•×“×” ×¢×©×¨×•× ×™×ª
                numbers = re.findall(r'\d+\.?\d*', text.replace(',', ''))
                if numbers:
                    price = numbers[0]
                    break

        # × ×™×§×•×™ ×¡×•×¤×™ ×œ××—×™×¨
        if price:
            # ×”×¡×¨×ª ×ª×•×•×™× ×©××™× × ××¡×¤×¨×™× (×›××• â‚ª ××• ×¤×¡×™×§×™×)
            price = str(price).replace(',', '').replace('â‚ª', '').strip()
            price = float(re.findall(r'\d+\.?\d*', price)[0])

        return {
            "timestamp": datetime.now(TZ_ISRAEL).strftime("%Y-%m-%d %H:%M:%S"),
            "price": price if price else 0,
            "title": title.strip() if title else "××•×¦×¨ ×œ× ××–×•×”×”",
            "url": product_url
        }
    except Exception as e:
        print(f"Error scraping {product_url}: {e}")
        return None

def update_database(new_entries):
    data = []
    if os.path.exists(DATA_FILE):
        with open(DATA_FILE, 'r', encoding='utf-8') as f:
            try:
                data = json.load(f)
            except json.JSONDecodeError:
                data = []
    
    data.extend(new_entries)
    with open(DATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    return data

def generate_readme(all_data):
    if not all_data:
        return

    df = pd.DataFrame(all_data)
    readme_content = "# ğŸ¤– ×‘×•×˜ ××¢×§×‘ ××—×™×¨×™× ××•×˜×•××˜×™\n\n"
    readme_content += f"**×¢×“×›×•×Ÿ ××—×¨×•×Ÿ:** {datetime.now(TZ_ISRAEL).strftime('%Y-%m-%d %H:%M:%S')}\n\n"

    for url in df['url'].unique():
        p_df = df[df['url'] == url]
        latest = p_df.iloc[-1]
        
        # ×—×™×©×•×‘ ×©×™× ×•×™
        diff_text = "â– ×™×¦×™×‘"
        if len(p_df) > 1:
            prev_price = p_df.iloc[-2]['price']
            if latest['price'] > 0 and prev_price > 0:
                if latest['price'] < prev_price:
                    diff_text = f"ğŸ”» ×™×¨×™×“×” ×©×œ â‚ª{round(prev_price - latest['price'], 2)}"
                elif latest['price'] > prev_price:
                    diff_text = f"ğŸ”º ×¢×œ×™×™×” ×©×œ â‚ª{round(latest['price'] - prev_price(), 2)}"

        status_icon = "âœ…" if latest['price'] > 0 else "âŒ ×ª×§×œ×” ×‘×¡×¨×™×§×”"
        
        readme_content += f"### {status_icon} [{latest['title']}]({url})\n"
        readme_content += f"- **××—×™×¨ × ×•×›×—×™:** `â‚ª{latest['price']}`\n"
        readme_content += f"- **××¦×‘:** {diff_text}\n"
        readme_content += f"- **×”×›×™ ×–×•×œ ×©× ×¦×¤×”:** â‚ª{p_df[p_df['price'] > 0]['price'].min() if not p_df[p_df['price'] > 0].empty else 0}\n\n"
        
        readme_content += "| ×ª××¨×™×š | ××—×™×¨ |\n|---|---|\n"
        for _, row in p_df.tail(5).iloc[::-1].iterrows():
            readme_content += f"| {row['timestamp']} | â‚ª{row['price']} |\n"
        readme_content += "\n---\n"

    with open(README_FILE, 'w', encoding='utf-8') as f:
        f.write(readme_content)

if __name__ == "__main__":
    if not os.path.exists(URLS_FILE):
        with open(URLS_FILE, 'w') as f: f.write("") # ×™×¦×™×¨×ª ×§×•×‘×¥ ×¨×™×§ ×× ×œ× ×§×™×™×

    with open(URLS_FILE, 'r') as f:
        urls = [line.strip() for line in f if line.strip() and not line.startswith("#")]

    results = []
    for url in urls:
        print(f"ğŸ” ×‘×•×“×§: {url}")
        res = get_product_data(url)
        if res:
            results.append(res)
    
    if results:
        full_data = update_database(results)
        generate_readme(full_data)
        print("âœ… ×”×¡×¨×™×§×” ×”×•×©×œ××”.")
